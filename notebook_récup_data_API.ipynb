{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des tweets et articles en rapport au marché boursier US via les API \n",
    "\n",
    "**Projet Python - 2A ENSAE** . \n",
    "\n",
    "Elena Loumagne / Jérémie Darracq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Ce notebook a été créé extraire des données via des requêtes envoyées aux API. Pour obtenir les clés d'accès aux API, nous avons eu besoin de créer un compte FinHub et un compte twitter developper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elelo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pour la récupération des données\n",
    "\n",
    "#%pip install finnhub-python\n",
    "import finnhub\n",
    "import pandas as pd \n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "## Pour le preprocessing\n",
    "\n",
    "#%pip install demoji\n",
    "#%pip install nltk\n",
    "\n",
    "import demoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Finhub\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API finhub et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui transforme les données fournit par l'API en un dictionnaire \n",
    "\n",
    "def get_data(article):\n",
    "    data = {\n",
    "        'date': time.strftime(\"%D %H:%M\", time.localtime(article['datetime'])),\n",
    "        'headline': article['headline'],\n",
    "        'company': article['related'],\n",
    "        'abstract': article['summary'],\n",
    "        'source': article['source']\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finnhub_client = finnhub.Client(api_key=\"ce74bdiad3iakcsvp120ce74bdiad3iakcsvp12g\")  ## Fonction d'appel à l'API\n",
    "\n",
    "start_date = \"2022-10-01\" ## Date du début de la requête \n",
    "end_date = \"2022-12-05\"  ## Date de fin de la requête \n",
    "\n",
    "## Liste des companies du SP500 apparaissant dans les articles \n",
    "\n",
    "List_company = ['AMZN','AAPL','MSFT','META','JPM','JNJ','GOOGL','PFE','BAC','NFLX','MA','MCD','GS','INTC','TMUS','WMT','CBOE','MRK','WFC','BA','MRNA','NDAQ','NKE','SLB','TSLA','VZ','T','AXP','BRK.B','CAT','CVX','KO','EA','FDX','GE','IBM','UNH','XOM','NVDA','PEP','COST','DIS','PM','ATVI','ADBE','AAL','AIG','BIIB','AVGO','COF']\n",
    "\n",
    "## Pour chaque companie nous effectuons un appel à l'API \n",
    "\n",
    "df = pd.DataFrame() \n",
    "for symbol in List_company :\n",
    "    response = finnhub_client.company_news(symbol, _from= start_date, to=end_date)\n",
    "    for article in response:\n",
    "        row = get_data(article) \n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "## On stock la base de donné fournit par Finhub \n",
    "\n",
    "df.to_csv(\"Data/data_finhub.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On modifie le format de la date \n",
    "df[\"date\"]=df[\"date\"].apply(lambda x : x[0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des articles Finhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fonction qui nettoie le texte\n",
    "\n",
    "def transform(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On nettoie les résumés des articles \n",
    "df[\"abstract_clean\"]=df[\"abstract\"].apply(lambda x : transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  fonction qui supprime les petits mots non pertinent pour l'analse ( ex: you,the )\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def retrait_sw(text):\n",
    "    return ' '.join([word for word in text.split() if word.casefold() not in stopwords ])\n",
    "\n",
    "df[\"abstract_clean\"] = df[\"abstract_clean\"].apply(retrait_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>company</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>abstract_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/05/22</td>\n",
       "      <td>Why Amazon, Okta, and Roku Stocks All Slumped ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>A report on the state of the service sector fu...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>report state service sector fueled fears feder...</td>\n",
       "      <td>report state servic sector fuel fear feder res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/05/22</td>\n",
       "      <td>Apple and Amazon resume Twitter advertising</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Yahoo Finance tech editor Dan Howley outlines ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>yahoo finance tech editor dan howley outlines ...</td>\n",
       "      <td>yahoo financ tech editor dan howley outlin app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/05/22</td>\n",
       "      <td>Amazon's (AMZN) AWS Gets Selected by Yahoo, Bo...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon's (AMZN) AWS is chosen by Yahoo as the ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>amazon (amzn) aws chosen yahoo preferred publi...</td>\n",
       "      <td>amazon (amzn) aw chosen yahoo prefer public cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/05/22</td>\n",
       "      <td>Amazon.com Inc. stock underperforms Monday whe...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Shares of Amazon.com Inc. shed 3.31% to $91.01...</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>shares amazoncom inc shed $ monday, proved all...</td>\n",
       "      <td>share amazoncom inc shed $ monday, prove all-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/05/22</td>\n",
       "      <td>After explosive growth, Philadelphia tech star...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Triumph Technology Solutions, which has experi...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>triumph technology solutions, experienced year...</td>\n",
       "      <td>triumph technolog solutions, experienc year-ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>10/06/22</td>\n",
       "      <td>Former Amazon cloud engineer gets probation fo...</td>\n",
       "      <td>COF</td>\n",
       "      <td>The hack compromised about 140,000 Social Secu...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>hack compromised , social security numbers , b...</td>\n",
       "      <td>hack compromis , social secur number , bank ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10834</th>\n",
       "      <td>10/05/22</td>\n",
       "      <td>Capital One Financial Corporation to Webcast C...</td>\n",
       "      <td>COF</td>\n",
       "      <td>On Thursday, October 27, 2022, at approximatel...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>thursday, october , , approximately : pm easte...</td>\n",
       "      <td>thursday, octob , , approxim : pm eastern time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>10/04/22</td>\n",
       "      <td>Why Is RIVN Stock Up Today? Rivian Confirmed 2...</td>\n",
       "      <td>COF</td>\n",
       "      <td>RIVN stock is up after Rivian announced it pro...</td>\n",
       "      <td>InvestorPlace</td>\n",
       "      <td>rivn stock rivian announced produced , vehicle...</td>\n",
       "      <td>rivn stock rivian announc produc , vehicl trac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10836</th>\n",
       "      <td>10/03/22</td>\n",
       "      <td>Q3 2022 Bank Analysts' Top 3 Picks, Earnings E...</td>\n",
       "      <td>COF</td>\n",
       "      <td>Big bank stocks are down 27% on average, but a...</td>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>big bank stocks average, analysts see sunny da...</td>\n",
       "      <td>big bank stock average, analyst see sunni day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10837</th>\n",
       "      <td>10/03/22</td>\n",
       "      <td>CAPITAL ONE FINANCIAL CORP : Other Events, Fin...</td>\n",
       "      <td>COF</td>\n",
       "      <td>Item 8.01 Other Events.On October 1, 2022, Cap...</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>item eventson october , , capital one financia...</td>\n",
       "      <td>item eventson octob , , capit one financi corp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10838 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                           headline company  \\\n",
       "0      12/05/22  Why Amazon, Okta, and Roku Stocks All Slumped ...    AMZN   \n",
       "1      12/05/22        Apple and Amazon resume Twitter advertising    AMZN   \n",
       "2      12/05/22  Amazon's (AMZN) AWS Gets Selected by Yahoo, Bo...    AMZN   \n",
       "3      12/05/22  Amazon.com Inc. stock underperforms Monday whe...    AMZN   \n",
       "4      12/05/22  After explosive growth, Philadelphia tech star...    AMZN   \n",
       "...         ...                                                ...     ...   \n",
       "10833  10/06/22  Former Amazon cloud engineer gets probation fo...     COF   \n",
       "10834  10/05/22  Capital One Financial Corporation to Webcast C...     COF   \n",
       "10835  10/04/22  Why Is RIVN Stock Up Today? Rivian Confirmed 2...     COF   \n",
       "10836  10/03/22  Q3 2022 Bank Analysts' Top 3 Picks, Earnings E...     COF   \n",
       "10837  10/03/22  CAPITAL ONE FINANCIAL CORP : Other Events, Fin...     COF   \n",
       "\n",
       "                                                abstract         source  \\\n",
       "0      A report on the state of the service sector fu...          Yahoo   \n",
       "1      Yahoo Finance tech editor Dan Howley outlines ...          Yahoo   \n",
       "2      Amazon's (AMZN) AWS is chosen by Yahoo as the ...          Yahoo   \n",
       "3      Shares of Amazon.com Inc. shed 3.31% to $91.01...    MarketWatch   \n",
       "4      Triumph Technology Solutions, which has experi...          Yahoo   \n",
       "...                                                  ...            ...   \n",
       "10833  The hack compromised about 140,000 Social Secu...          Yahoo   \n",
       "10834  On Thursday, October 27, 2022, at approximatel...          Yahoo   \n",
       "10835  RIVN stock is up after Rivian announced it pro...  InvestorPlace   \n",
       "10836  Big bank stocks are down 27% on average, but a...   SeekingAlpha   \n",
       "10837  Item 8.01 Other Events.On October 1, 2022, Cap...        Finnhub   \n",
       "\n",
       "                                          abstract_clean  \\\n",
       "0      report state service sector fueled fears feder...   \n",
       "1      yahoo finance tech editor dan howley outlines ...   \n",
       "2      amazon (amzn) aws chosen yahoo preferred publi...   \n",
       "3      shares amazoncom inc shed $ monday, proved all...   \n",
       "4      triumph technology solutions, experienced year...   \n",
       "...                                                  ...   \n",
       "10833  hack compromised , social security numbers , b...   \n",
       "10834  thursday, october , , approximately : pm easte...   \n",
       "10835  rivn stock rivian announced produced , vehicle...   \n",
       "10836  big bank stocks average, analysts see sunny da...   \n",
       "10837  item eventson october , , capital one financia...   \n",
       "\n",
       "                                        abstract_stemmed  \n",
       "0      report state servic sector fuel fear feder res...  \n",
       "1      yahoo financ tech editor dan howley outlin app...  \n",
       "2      amazon (amzn) aw chosen yahoo prefer public cl...  \n",
       "3      share amazoncom inc shed $ monday, prove all-a...  \n",
       "4      triumph technolog solutions, experienc year-ov...  \n",
       "...                                                  ...  \n",
       "10833  hack compromis , social secur number , bank ac...  \n",
       "10834  thursday, octob , , approxim : pm eastern time...  \n",
       "10835  rivn stock rivian announc produc , vehicl trac...  \n",
       "10836  big bank stock average, analyst see sunni day ...  \n",
       "10837  item eventson octob , , capit one financi corp...  \n",
       "\n",
       "[10838 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## On \"stem\" les résumés des articles pour ne garder que la racine des mots \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "df[\"abstract_stemmed\"]=df[\"abstract_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons récupéré et nettoyé tous les abstracts des articles. Nous pouvons dès à présent les stocker dans un fichier .csv pour pouvoir les analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/data_finhub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Twitter\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API twitter et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir un maximum de tweet, nous avons dû faire la demande pour mettre à jour notre compte twitter developper en un compte elevated. Cela permet d'obtenir un nombre de tweets plus important.  \n",
    "Malheureseument, la période d'extraction des tweets avec l'API ne s'étend qu'aux 7 derniers jours. C'est pourquoi nous utiliserons dans un second temps les tweets pour la construction de notre modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAEHijwEAAAAA5k533gmjemyLZvGcHJ85KptB2ag%3DakjvNX75aeG15S5hKt8tVPniNlrXN0DihoVURgMjmoXXcr7e6M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tweet):\n",
    "    data = {\n",
    "        'date': tweet['created_at'],\n",
    "        'text': tweet['text']\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION :** le code pour récupérer les tweets financiers des 7 derniers jours prend un dizaine de minutes à tourner. Si vous souhaitez vous épargner ce temps d'attente, vous pouvez directement passer à l'étape suivante en important la base de tweets non nettoyée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel à l'API\n",
    "endpoint = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "headers = {'authorization': f'Bearer {BEARER_TOKEN}'}\n",
    "params = {\n",
    "    'query': '(VIX OR S&P 500 OR CBOE OR investement OR stock market OR Federal Reserve Bank OR stock price OR inflation OR bonds ) (lang:en)', # mots présents dans les tweets\n",
    "    'max_results': '100', #  on récupère 100 tweets à chaque appel à l'API. C'est le nombre maximal de tweets que l'on peut obtenir avec notre compte.\n",
    "    'tweet.fields': 'created_at,lang'\n",
    "}\n",
    "\n",
    "\n",
    "dtformat = '%Y-%m-%dT%H:%M:%S.000Z'  # le format de la date recquis par l'API Twitter\n",
    "\n",
    "def time_travel(now, mins):\n",
    "    '''\n",
    "    Permet de modifier la date en voyageant de 'mins' minutes dans le passé \n",
    "\n",
    "    'now : str \n",
    "        date actuelle\n",
    "    \n",
    "    'mins : int\n",
    "        minutes à soustraire\n",
    "    '''\n",
    "\n",
    "    now = datetime.strptime(now, dtformat)\n",
    "    back_in_time = now - timedelta(minutes=mins)\n",
    "    return back_in_time.strftime(dtformat)\n",
    "    \n",
    "now = datetime.now() # date actuel\n",
    "last_week = now - timedelta(days=7)  # date de fin\n",
    "now = now.strftime(dtformat)  # convertit la date au format de l'API\n",
    "\n",
    "\n",
    "now=time_travel(now,60)\n",
    "df_tweet = pd.DataFrame()  \n",
    "\n",
    "time.sleep(20) # on attend 20s pour déclencher l'appel à l'API\n",
    "\n",
    "\n",
    "while True:\n",
    "    if datetime.strptime(now, dtformat) < last_week:\n",
    "        # si on atteint les 7 jours, on sort de la boucle\n",
    "        break\n",
    "    pre60 = time_travel(now, 30)  \n",
    "    \n",
    "    params['start_time'] = pre60\n",
    "    params['end_time'] = now\n",
    "    response = requests.get(endpoint,\n",
    "                            params=params,\n",
    "                            headers=headers)  # envoie de la requête\n",
    "    now = pre60  # on voyage 60 min avant pour récupérer un maximum de tweet sur chaque heure\n",
    "\n",
    "    # on ajoute les tweets à notre df\n",
    "    for tweet in response.json()['data']:\n",
    "        row = get_data(tweet)  \n",
    "        df_tweet = df_tweet.append(row, ignore_index=True)\n",
    "\n",
    "df_tweet.to_csv(\"Data/tweet_not_cleaned.csv\",index=False) # enregistrement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des tweets pas nettoyées :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=pd.read_csv(\"Data/tweet_not_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on supprime tous les pseudos qui comportent le mot vix car cela ne correspond pas aux tweet sur la finance\n",
    "for index,row in df_tweet.iterrows():\n",
    "    tweet=row[\"text\"]\n",
    "    real_pseudo=[]\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        if '@' in word:\n",
    "            real_pseudo.append(word)\n",
    "\n",
    "    vix_in_pseudo=[\"vix\" in pseudo.lower() for pseudo in real_pseudo]\n",
    "\n",
    "    if True in vix_in_pseudo:\n",
    "        df_tweet.drop(index, inplace=True)\n",
    "\n",
    "df_tweet.reset_index(inplace=True,drop=True)\n",
    "\n",
    "## on modifie la date \n",
    "df_tweet[\"date\"]=df_tweet[\"date\"].apply(lambda x : x[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui nettoie les tweets\n",
    "\n",
    "def transform_tweet(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    #retirer les liens\n",
    "    for item in re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # enlever le retour à la ligne\n",
    "    texte = texte.replace(\"\\n\", \" \").replace(\"\\r\", \"\") \n",
    "    # supprimer \",\", \"!\", \"?\", \"%\", \"(\",\")\",\"/\",'\"', \"$\",\"£\", \"_\", \"-\", \"+\", \"*\", \"µ\", \":\",\"&,\"§\" \n",
    "    texte = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"\\$\\£\\+\\*\\µ,\\:\\&\\§]\", \" \", texte) \n",
    "    # retirer les hashtags #\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\") \n",
    "    # retirer les personnes tagées\n",
    "    for item in re.compile(\"([@]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les adresses mail\n",
    "    for item in re.findall('\\S+@\\S+', texte) :\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retire les mots contenant des chiffres\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    # retirer les emojis\n",
    "    for item in demoji.findall(texte):\n",
    "        texte=texte.replace(item,\"\")\n",
    "    return texte\n",
    "\n",
    "df_tweet[\"tweet_clean\"]=df_tweet[\"text\"].apply(lambda x : transform_tweet(x))\n",
    "\n",
    "## on retire les stopswords\n",
    "\n",
    "df_tweet[\"tweet_clean\"] = df_tweet[\"tweet_clean\"].apply(retrait_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on \"stem\" les tweets\n",
    "df_tweet[\"tweet_stemmed\"]=df_tweet[\"tweet_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons dès à présent finit le nettoyage des tweets. Nous les enregistrons dans le fichier .csv pour pouvoir les utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.to_csv(\"Data/tweets.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "809c05bf197248844069ca010303a407c57ade7aa07b41750c0b87641ddc144b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
