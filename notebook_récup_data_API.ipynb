{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des tweets et articles en rapport au marché boursier US via les API \n",
    "\n",
    "**Projet Python - 2A ENSAE** . \n",
    "\n",
    "Elena Loumagne / Jérémie Darracq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Ce notebook a été créé extraire des données via des requêtes envoyées aux API. Pour obtenir les clés d'accès aux API, nous avons eu besoin de créer un compte FinHub et un compte twitter developper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremiedarracq/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeremiedarracq/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jeremiedarracq/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeremiedarracq/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pour la récupération des données\n",
    "\n",
    "#%pip install finnhub-python\n",
    "import finnhub\n",
    "import pandas as pd \n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "## Pour le preprocessing\n",
    "\n",
    "#%pip install demoji\n",
    "#%pip install nltk\n",
    "\n",
    "import demoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Finhub\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API finhub et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui transforme les données fournit par l'API en un dictionnaire \n",
    "\n",
    "def get_data(article):\n",
    "    data = {\n",
    "        'date': time.strftime(\"%D %H:%M\", time.localtime(article['datetime'])),\n",
    "        'headline': article['headline'],\n",
    "        'company': article['related'],\n",
    "        'abstract': article['summary'],\n",
    "        'source': article['source']\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finnhub_client = finnhub.Client(api_key=\"ce74bdiad3iakcsvp120ce74bdiad3iakcsvp12g\")  ## Fonction d'appel à l'API\n",
    "\n",
    "start_date = \"2022-10-05\" ## Date du début de la requête \n",
    "end_date = \"2022-12-12\"  ## Date de fin de la requête \n",
    "\n",
    "## Liste des companies du SP500 apparaissant dans les articles \n",
    "\n",
    "List_company = ['AMZN','AAPL','MSFT','META','JPM','JNJ','GOOGL','PFE','BAC','NFLX','MA','MCD','GS','INTC','TMUS','WMT','CBOE','MRK','WFC','BA','MRNA','NDAQ','NKE','SLB','TSLA','VZ','T','AXP','BRK.B','CAT','CVX','KO','EA','FDX','GE','IBM','UNH','XOM','NVDA','PEP','COST','DIS','PM','ATVI','ADBE','AAL','AIG','BIIB','AVGO','COF']\n",
    "\n",
    "## Pour chaque companie nous effectuons un appel à l'API \n",
    "\n",
    "df = pd.DataFrame() \n",
    "for symbol in List_company :\n",
    "    response = finnhub_client.company_news(symbol, _from= start_date, to=end_date)\n",
    "    for article in response:\n",
    "        row = get_data(article) \n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "## On stock la base de donné fournit par Finhub \n",
    "\n",
    "df.to_csv(\"Data/data_finhub.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On modifie le format de la date \n",
    "df[\"date\"]=df[\"date\"].apply(lambda x : x[0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des articles Finhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fonction qui nettoie le texte\n",
    "\n",
    "def transform(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On nettoie les résumés des articles \n",
    "df[\"abstract_clean\"]=df[\"abstract\"].apply(lambda x : transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  fonction qui supprime les petits mots non pertinent pour l'analse ( ex: you,the )\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def retrait_sw(text):\n",
    "    return ' '.join([word for word in text.split() if word.casefold() not in stopwords ])\n",
    "\n",
    "df[\"abstract_clean\"] = df[\"abstract_clean\"].apply(retrait_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On \"stem\" les résumés des articles pour ne garder que la racine des mots \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "df[\"abstract_stemmed\"]=df[\"abstract_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>abstract_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>Cboe Global Markets, Inc. (Cboe: CBOE), a lead...</td>\n",
       "      <td>CBOE</td>\n",
       "      <td>10/05/22</td>\n",
       "      <td>Cboe Global Markets Reports Trading Volume for...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>cboe global markets, inc (cboe: cboe), leading...</td>\n",
       "      <td>cboe global markets, inc (cboe: cboe), lead pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10353</th>\n",
       "      <td>NEW YORK, October 05, 2022--American Internati...</td>\n",
       "      <td>AIG</td>\n",
       "      <td>10/05/22</td>\n",
       "      <td>AIG to Report Third Quarter 2022 Financial Res...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>new york, october , --american international g...</td>\n",
       "      <td>new york, octob , --american intern group, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>Sony, Microsoft, Nintendo and Electronic Arts ...</td>\n",
       "      <td>EA</td>\n",
       "      <td>10/05/22</td>\n",
       "      <td>The Zacks Analyst Blog Highlights Sony, Micros...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>sony, microsoft, nintendo electronic arts part...</td>\n",
       "      <td>sony, microsoft, nintendo electron art part za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>Bearish flow noted in Ele... EA</td>\n",
       "      <td>EA</td>\n",
       "      <td>10/05/22</td>\n",
       "      <td>Electronic Arts put volume heavy and direction...</td>\n",
       "      <td>Thefly.com</td>\n",
       "      <td>bearish flow noted ele ea</td>\n",
       "      <td>bearish flow note ele ea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>Cboe Global Markets, Inc. (Cboe: CBOE), a lead...</td>\n",
       "      <td>CBOE</td>\n",
       "      <td>10/05/22</td>\n",
       "      <td>Cboe Joins Pyth Network to Bring Market Data t...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>cboe global markets, inc (cboe: cboe), leading...</td>\n",
       "      <td>cboe global markets, inc (cboe: cboe), lead pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>Why investing for the long run, especially if ...</td>\n",
       "      <td>CBOE</td>\n",
       "      <td>12/12/22</td>\n",
       "      <td>If You Invested $1000 in CBOE Global a Decade ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>investing long run, especially buy certain pop...</td>\n",
       "      <td>invest long run, especi buy certain popular st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>Sales growth is vital for the survival of a bu...</td>\n",
       "      <td>CBOE</td>\n",
       "      <td>12/12/22</td>\n",
       "      <td>5 Stocks With Solid Sales Growth to Fight Econ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>sales growth vital survival business, even eco...</td>\n",
       "      <td>sale growth vital surviv business, even econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>RAHWAY, N.J., December 12, 2022--Merck Begins ...</td>\n",
       "      <td>MRK</td>\n",
       "      <td>12/12/22</td>\n",
       "      <td>Merck Begins Tender Offer to Acquire Imago Bio...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>rahway, nj, december , --merck begins tender o...</td>\n",
       "      <td>rahway, nj, decemb , --merck begin tender offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>The sequel to movie box office champion \"Avata...</td>\n",
       "      <td>DIS</td>\n",
       "      <td>12/12/22</td>\n",
       "      <td>Expensive 'Avatar' sequel faces transformed mo...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>sequel movie box office champion \"avatar\" arri...</td>\n",
       "      <td>sequel movi box offic champion \"avatar\" arriv ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electric vehicle stocks have had a tough time ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>12/12/22</td>\n",
       "      <td>Rivian Hits the Brakes on Mercedes Deal: Will ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>electric vehicle stocks tough time , companies...</td>\n",
       "      <td>electr vehicl stock tough time , compani acros...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10907 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract company      date  \\\n",
       "3817   Cboe Global Markets, Inc. (Cboe: CBOE), a lead...    CBOE  10/05/22   \n",
       "10353  NEW YORK, October 05, 2022--American Internati...     AIG  10/05/22   \n",
       "7272   Sony, Microsoft, Nintendo and Electronic Arts ...      EA  10/05/22   \n",
       "7273                     Bearish flow noted in Ele... EA      EA  10/05/22   \n",
       "3818   Cboe Global Markets, Inc. (Cboe: CBOE), a lead...    CBOE  10/05/22   \n",
       "...                                                  ...     ...       ...   \n",
       "3726   Why investing for the long run, especially if ...    CBOE  12/12/22   \n",
       "3727   Sales growth is vital for the survival of a bu...    CBOE  12/12/22   \n",
       "3823   RAHWAY, N.J., December 12, 2022--Merck Begins ...     MRK  12/12/22   \n",
       "9126   The sequel to movie box office champion \"Avata...     DIS  12/12/22   \n",
       "0      Electric vehicle stocks have had a tough time ...    AMZN  12/12/22   \n",
       "\n",
       "                                                headline      source  \\\n",
       "3817   Cboe Global Markets Reports Trading Volume for...       Yahoo   \n",
       "10353  AIG to Report Third Quarter 2022 Financial Res...       Yahoo   \n",
       "7272   The Zacks Analyst Blog Highlights Sony, Micros...       Yahoo   \n",
       "7273   Electronic Arts put volume heavy and direction...  Thefly.com   \n",
       "3818   Cboe Joins Pyth Network to Bring Market Data t...       Yahoo   \n",
       "...                                                  ...         ...   \n",
       "3726   If You Invested $1000 in CBOE Global a Decade ...       Yahoo   \n",
       "3727   5 Stocks With Solid Sales Growth to Fight Econ...       Yahoo   \n",
       "3823   Merck Begins Tender Offer to Acquire Imago Bio...       Yahoo   \n",
       "9126   Expensive 'Avatar' sequel faces transformed mo...       Yahoo   \n",
       "0      Rivian Hits the Brakes on Mercedes Deal: Will ...       Yahoo   \n",
       "\n",
       "                                          abstract_clean  \\\n",
       "3817   cboe global markets, inc (cboe: cboe), leading...   \n",
       "10353  new york, october , --american international g...   \n",
       "7272   sony, microsoft, nintendo electronic arts part...   \n",
       "7273                           bearish flow noted ele ea   \n",
       "3818   cboe global markets, inc (cboe: cboe), leading...   \n",
       "...                                                  ...   \n",
       "3726   investing long run, especially buy certain pop...   \n",
       "3727   sales growth vital survival business, even eco...   \n",
       "3823   rahway, nj, december , --merck begins tender o...   \n",
       "9126   sequel movie box office champion \"avatar\" arri...   \n",
       "0      electric vehicle stocks tough time , companies...   \n",
       "\n",
       "                                        abstract_stemmed  \n",
       "3817   cboe global markets, inc (cboe: cboe), lead pr...  \n",
       "10353  new york, octob , --american intern group, inc...  \n",
       "7272   sony, microsoft, nintendo electron art part za...  \n",
       "7273                            bearish flow note ele ea  \n",
       "3818   cboe global markets, inc (cboe: cboe), lead pr...  \n",
       "...                                                  ...  \n",
       "3726   invest long run, especi buy certain popular st...  \n",
       "3727   sale growth vital surviv business, even econom...  \n",
       "3823   rahway, nj, decemb , --merck begin tender offe...  \n",
       "9126   sequel movi box offic champion \"avatar\" arriv ...  \n",
       "0      electr vehicl stock tough time , compani acros...  \n",
       "\n",
       "[10907 rows x 7 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### On supprime les dates non inclus dans période voulu (problème de filtrage de l'API finhub)\n",
    "df_trier = df.sort_values(by=\"date\")\n",
    "df_trier.drop(df_trier.index[0:6],inplace=True)\n",
    "df = df_trier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons récupéré et nettoyé tous les abstracts des articles. Nous pouvons dès à présent les stocker dans un fichier .csv pour pouvoir les analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/data_finhub.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Twitter\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API twitter et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir un maximum de tweet, nous avons dû faire la demande pour mettre à jour notre compte twitter developper en un compte elevated. Cela permet d'obtenir un nombre de tweets plus important.  \n",
    "Malheureseument, la période d'extraction des tweets avec l'API ne s'étend qu'aux 7 derniers jours. C'est pourquoi nous utiliserons dans un second temps les tweets pour la construction de notre modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAEHijwEAAAAA5k533gmjemyLZvGcHJ85KptB2ag%3DakjvNX75aeG15S5hKt8tVPniNlrXN0DihoVURgMjmoXXcr7e6M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tweet):\n",
    "    data = {\n",
    "        'date': tweet['created_at'],\n",
    "        'text': tweet['text']\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION :** le code pour récupérer les tweets financiers des 7 derniers jours prend un dizaine de minutes à tourner. Si vous souhaitez vous épargner ce temps d'attente, vous pouvez directement passer à l'étape suivante en important la base de tweets non nettoyée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f255150eafd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     response = requests.get(endpoint,\n\u001b[0m\u001b[1;32m     48\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                             headers=headers)  # envoie de la requête\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Appel à l'API\n",
    "endpoint = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "headers = {'authorization': f'Bearer {BEARER_TOKEN}'}\n",
    "params = {\n",
    "    'query': '(VIX OR S&P 500 OR CBOE OR investement OR stock market OR Federal Reserve Bank OR stock price OR inflation OR bonds ) (lang:en)', # mots présents dans les tweets\n",
    "    'max_results': '100', #  on récupère 100 tweets à chaque appel à l'API. C'est le nombre maximal de tweets que l'on peut obtenir avec notre compte.\n",
    "    'tweet.fields': 'created_at,lang'\n",
    "}\n",
    "\n",
    "\n",
    "dtformat = '%Y-%m-%dT%H:%M:%S.000Z'  # le format de la date recquis par l'API Twitter\n",
    "\n",
    "def time_travel(now, mins):\n",
    "    '''\n",
    "    Permet de modifier la date en voyageant de 'mins' minutes dans le passé \n",
    "\n",
    "    'now : str \n",
    "        date actuelle\n",
    "    \n",
    "    'mins : int\n",
    "        minutes à soustraire\n",
    "    '''\n",
    "\n",
    "    now = datetime.strptime(now, dtformat)\n",
    "    back_in_time = now - timedelta(minutes=mins)\n",
    "    return back_in_time.strftime(dtformat)\n",
    "    \n",
    "now = datetime.now() # date actuel\n",
    "last_week = now - timedelta(days=7)  # date de fin\n",
    "now = now.strftime(dtformat)  # convertit la date au format de l'API\n",
    "\n",
    "\n",
    "now=time_travel(now,60)\n",
    "df_tweet = pd.DataFrame()  \n",
    "\n",
    "time.sleep(20) # on attend 20s pour déclencher l'appel à l'API\n",
    "\n",
    "\n",
    "while True:\n",
    "    if datetime.strptime(now, dtformat) < last_week:\n",
    "        # si on atteint les 7 jours, on sort de la boucle\n",
    "        break\n",
    "    pre60 = time_travel(now, 30)  \n",
    "    \n",
    "    params['start_time'] = pre60\n",
    "    params['end_time'] = now\n",
    "    response = requests.get(endpoint,\n",
    "                            params=params,\n",
    "                            headers=headers)  # envoie de la requête\n",
    "    now = pre60  # on voyage 60 min avant pour récupérer un maximum de tweet sur chaque heure\n",
    "\n",
    "    # on ajoute les tweets à notre df\n",
    "    for tweet in response.json()['data']:\n",
    "        row = get_data(tweet)  \n",
    "        df_tweet = df_tweet.append(row, ignore_index=True)\n",
    "\n",
    "df_tweet.to_csv(\"Data/tweet_not_cleaned.csv\",index=False) # enregistrement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des tweets pas nettoyées :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=pd.read_csv(\"Data/tweet_not_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on supprime tous les pseudos qui comportent le mot vix car cela ne correspond pas aux tweet sur la finance\n",
    "for index,row in df_tweet.iterrows():\n",
    "    tweet=row[\"text\"]\n",
    "    real_pseudo=[]\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        if '@' in word:\n",
    "            real_pseudo.append(word)\n",
    "\n",
    "    vix_in_pseudo=[\"vix\" in pseudo.lower() for pseudo in real_pseudo]\n",
    "\n",
    "    if True in vix_in_pseudo:\n",
    "        df_tweet.drop(index, inplace=True)\n",
    "\n",
    "df_tweet.reset_index(inplace=True,drop=True)\n",
    "\n",
    "## on modifie la date \n",
    "df_tweet[\"date\"]=df_tweet[\"date\"].apply(lambda x : x[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui nettoie les tweets\n",
    "\n",
    "def transform_tweet(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    #retirer les liens\n",
    "    for item in re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # enlever le retour à la ligne\n",
    "    texte = texte.replace(\"\\n\", \" \").replace(\"\\r\", \"\") \n",
    "    # supprimer \",\", \"!\", \"?\", \"%\", \"(\",\")\",\"/\",'\"', \"$\",\"£\", \"_\", \"-\", \"+\", \"*\", \"µ\", \":\",\"&,\"§\" \n",
    "    texte = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"\\$\\£\\+\\*\\µ,\\:\\&\\§]\", \" \", texte) \n",
    "    # retirer les hashtags #\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\") \n",
    "    # retirer les personnes tagées\n",
    "    for item in re.compile(\"([@]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les adresses mail\n",
    "    for item in re.findall('\\S+@\\S+', texte) :\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retire les mots contenant des chiffres\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    # retirer les emojis\n",
    "    for item in demoji.findall(texte):\n",
    "        texte=texte.replace(item,\"\")\n",
    "    return texte\n",
    "\n",
    "df_tweet[\"tweet_clean\"]=df_tweet[\"text\"].apply(lambda x : transform_tweet(x))\n",
    "\n",
    "## on retire les stopswords\n",
    "\n",
    "df_tweet[\"tweet_clean\"] = df_tweet[\"tweet_clean\"].apply(retrait_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on \"stem\" les tweets\n",
    "df_tweet[\"tweet_stemmed\"]=df_tweet[\"tweet_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons dès à présent finit le nettoyage des tweets. Nous les enregistrons dans le fichier .csv pour pouvoir les utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.to_csv(\"Data/tweets.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93257c97269dad6db5a0bed4f581319771fe589ffe8644d312475ce335138545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
