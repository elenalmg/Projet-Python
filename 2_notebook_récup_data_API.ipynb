{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des tweets et articles en rapport au marché boursier US via les API \n",
    "\n",
    "**Projet Python - 2A ENSAE** . \n",
    "\n",
    "Elena Loumagne / Jérémie Darracq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Ce notebook a été créé pour extraire des données via des requêtes envoyées aux API. Pour obtenir les clés d'accès aux API, nous avons eu besoin de créer un compte FinHub et un compte twitter developper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pour la récupération des données\n",
    "\n",
    "# Pour obtenir les clés cachés \n",
    "import yaml\n",
    "\n",
    "#%pip install finnhub-python\n",
    "import finnhub\n",
    "import pandas as pd \n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "## Pour le preprocessing\n",
    "\n",
    "#%pip install demoji\n",
    "#%pip install nltk\n",
    "\n",
    "import demoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Finhub\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API finhub et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui transforme les données fournit par l'API en un dictionnaire \n",
    "\n",
    "def get_data(article):\n",
    "    data = {\n",
    "        'date': time.strftime(\"%D %H:%M\", time.localtime(article['datetime'])),\n",
    "        'headline': article['headline'],\n",
    "        'company': article['related'],\n",
    "        'abstract': article['summary'],\n",
    "        'source': article['source']\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour accéder aux API, nous avons besoin de clés qui doivent rester privés. Nous les avons stockés dans un fichier annexe qui n'est pas visible sur le git. Nous importons donc le fichier qui comporte les clés d'accès aux API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = yaml.full_load(open('keys.yml')) # on charge le fichier contenant les clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnhub_client = finnhub.Client(api_key=keys['database']['key_finhub'])  ## Fonction d'appel à l'API\n",
    "\n",
    "start_date = \"2022-10-01\" ## Date du début de la requête \n",
    "end_date = \"2022-12-15\"  ## Date de fin de la requête \n",
    "\n",
    "## Liste des companies du SP500 apparaissant dans les articles \n",
    "\n",
    "List_company = ['AMZN','AAPL','MSFT','META','JPM','JNJ','GOOGL','PFE','BAC','NFLX','MA','MCD','GS','INTC','TMUS','WMT','CBOE','MRK','WFC','BA','MRNA','NDAQ','NKE','SLB','TSLA','VZ','T','AXP','BRK.B','CAT','CVX','KO','EA','FDX','GE','IBM','UNH','XOM','NVDA','PEP','COST','DIS','PM','ATVI','ADBE','AAL','AIG','BIIB','AVGO','COF']\n",
    "\n",
    "## Pour chaque compagnie nous effectuons un appel à l'API \n",
    "\n",
    "df = pd.DataFrame() \n",
    "for symbol in List_company :\n",
    "    response = finnhub_client.company_news(symbol, _from= start_date, to=end_date)\n",
    "    for article in response:\n",
    "        row = get_data(article) \n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "## On stock la base de donné fournit par Finhub \n",
    "\n",
    "df.to_csv(\"Data/data_finhub.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On modifie le format de la date \n",
    "df[\"date\"]=df[\"date\"].apply(lambda x : x[0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des articles Finhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fonction qui nettoie le texte\n",
    "\n",
    "def transform(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On nettoie les résumés des articles \n",
    "df[\"abstract_clean\"]=df[\"abstract\"].apply(lambda x : transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  fonction qui supprime les petits mots non pertinent pour l'analse ( ex: you,the )\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def retrait_sw(text):\n",
    "    return ' '.join([word for word in text.split() if word.casefold() not in stopwords ])\n",
    "\n",
    "df[\"abstract_clean\"] = df[\"abstract_clean\"].apply(retrait_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>company</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>abstract_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/15/22</td>\n",
       "      <td>Amazon goes TikTok. Here's how it will work.</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon has begun rolling out a short-form vide...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>amazon begun rolling short-form video photo fe...</td>\n",
       "      <td>amazon begun roll short-form video photo feed app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/15/22</td>\n",
       "      <td>Amazon reportedly reaches agreement with the E...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Yahoo Finance Live examines the latest develop...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>yahoo finance live examines latest development...</td>\n",
       "      <td>yahoo financ live examin latest develop amazon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/15/22</td>\n",
       "      <td>Why More Shoppers Are Turning to Reddit for Ho...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>This holiday season, as gift-buyers try to avo...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>holiday season, gift-buyers try avoid fake rev...</td>\n",
       "      <td>holiday season, gift-buy tri avoid fake review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/15/22</td>\n",
       "      <td>Here's Why I End Up Overspending at Target -- ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>There's a joke that it's impossible to make a ...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>joke impossible make shopping trip target with...</td>\n",
       "      <td>joke imposs make shop trip target without spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/15/22</td>\n",
       "      <td>Retail stocks including Macy's, Target get smo...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>An ugly day for retail stocks followed an ugly...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>ugly day retail stocks followed ugly retail sa...</td>\n",
       "      <td>ugli day retail stock follow ugli retail sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>10/06/22</td>\n",
       "      <td>Former Amazon cloud engineer gets probation fo...</td>\n",
       "      <td>COF</td>\n",
       "      <td>The hack compromised about 140,000 Social Secu...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>hack compromised , social security numbers , b...</td>\n",
       "      <td>hack compromis , social secur number , bank ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>10/05/22</td>\n",
       "      <td>Capital One Financial Corporation to Webcast C...</td>\n",
       "      <td>COF</td>\n",
       "      <td>On Thursday, October 27, 2022, at approximatel...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>thursday, october , , approximately : pm easte...</td>\n",
       "      <td>thursday, octob , , approxim : pm eastern time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>10/04/22</td>\n",
       "      <td>Why Is RIVN Stock Up Today? Rivian Confirmed 2...</td>\n",
       "      <td>COF</td>\n",
       "      <td>RIVN stock is up after Rivian announced it pro...</td>\n",
       "      <td>InvestorPlace</td>\n",
       "      <td>rivn stock rivian announced produced , vehicle...</td>\n",
       "      <td>rivn stock rivian announc produc , vehicl trac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>10/03/22</td>\n",
       "      <td>Q3 2022 Bank Analysts' Top 3 Picks, Earnings E...</td>\n",
       "      <td>COF</td>\n",
       "      <td>Big bank stocks are down 27% on average, but a...</td>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>big bank stocks average, analysts see sunny da...</td>\n",
       "      <td>big bank stock average, analyst see sunni day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>10/03/22</td>\n",
       "      <td>CAPITAL ONE FINANCIAL CORP : Other Events, Fin...</td>\n",
       "      <td>COF</td>\n",
       "      <td>Item 8.01 Other Events.On October 1, 2022, Cap...</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>item eventson october , , capital one financia...</td>\n",
       "      <td>item eventson octob , , capit one financi corp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                           headline company  \\\n",
       "0      12/15/22       Amazon goes TikTok. Here's how it will work.    AMZN   \n",
       "1      12/15/22  Amazon reportedly reaches agreement with the E...    AMZN   \n",
       "2      12/15/22  Why More Shoppers Are Turning to Reddit for Ho...    AMZN   \n",
       "3      12/15/22  Here's Why I End Up Overspending at Target -- ...    AMZN   \n",
       "4      12/15/22  Retail stocks including Macy's, Target get smo...    AMZN   \n",
       "...         ...                                                ...     ...   \n",
       "10881  10/06/22  Former Amazon cloud engineer gets probation fo...     COF   \n",
       "10882  10/05/22  Capital One Financial Corporation to Webcast C...     COF   \n",
       "10883  10/04/22  Why Is RIVN Stock Up Today? Rivian Confirmed 2...     COF   \n",
       "10884  10/03/22  Q3 2022 Bank Analysts' Top 3 Picks, Earnings E...     COF   \n",
       "10885  10/03/22  CAPITAL ONE FINANCIAL CORP : Other Events, Fin...     COF   \n",
       "\n",
       "                                                abstract         source  \\\n",
       "0      Amazon has begun rolling out a short-form vide...          Yahoo   \n",
       "1      Yahoo Finance Live examines the latest develop...          Yahoo   \n",
       "2      This holiday season, as gift-buyers try to avo...          Yahoo   \n",
       "3      There's a joke that it's impossible to make a ...          Yahoo   \n",
       "4      An ugly day for retail stocks followed an ugly...          Yahoo   \n",
       "...                                                  ...            ...   \n",
       "10881  The hack compromised about 140,000 Social Secu...          Yahoo   \n",
       "10882  On Thursday, October 27, 2022, at approximatel...          Yahoo   \n",
       "10883  RIVN stock is up after Rivian announced it pro...  InvestorPlace   \n",
       "10884  Big bank stocks are down 27% on average, but a...   SeekingAlpha   \n",
       "10885  Item 8.01 Other Events.On October 1, 2022, Cap...        Finnhub   \n",
       "\n",
       "                                          abstract_clean  \\\n",
       "0      amazon begun rolling short-form video photo fe...   \n",
       "1      yahoo finance live examines latest development...   \n",
       "2      holiday season, gift-buyers try avoid fake rev...   \n",
       "3      joke impossible make shopping trip target with...   \n",
       "4      ugly day retail stocks followed ugly retail sa...   \n",
       "...                                                  ...   \n",
       "10881  hack compromised , social security numbers , b...   \n",
       "10882  thursday, october , , approximately : pm easte...   \n",
       "10883  rivn stock rivian announced produced , vehicle...   \n",
       "10884  big bank stocks average, analysts see sunny da...   \n",
       "10885  item eventson october , , capital one financia...   \n",
       "\n",
       "                                        abstract_stemmed  \n",
       "0      amazon begun roll short-form video photo feed app  \n",
       "1      yahoo financ live examin latest develop amazon...  \n",
       "2      holiday season, gift-buy tri avoid fake review...  \n",
       "3      joke imposs make shop trip target without spen...  \n",
       "4      ugli day retail stock follow ugli retail sale ...  \n",
       "...                                                  ...  \n",
       "10881  hack compromis , social secur number , bank ac...  \n",
       "10882  thursday, octob , , approxim : pm eastern time...  \n",
       "10883  rivn stock rivian announc produc , vehicl trac...  \n",
       "10884  big bank stock average, analyst see sunni day ...  \n",
       "10885  item eventson octob , , capit one financi corp...  \n",
       "\n",
       "[10886 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## On \"stem\" les résumés des articles pour ne garder que la racine des mots \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "df[\"abstract_stemmed\"]=df[\"abstract_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons récupéré et nettoyé tous les abstracts des articles. Nous pouvons dès à présent les stocker dans un fichier .csv pour pouvoir les analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime des articles qui ne sont pas adéquats pour nos analyses futures \n",
    "\n",
    "df=df.drop(index=[6079,1559,4246,10146,7267])\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/data_finhub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des article via l'API de Twitter\n",
    "Dans cette section, on définit les fonctions permettant la connexion à l'API twitter et la récupération des données selon les termes clés, les dates et d'autres caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir un maximum de tweet, nous avons dû faire la demande pour mettre à jour notre compte twitter developper en un compte elevated. Cela permet d'obtenir un nombre de tweets plus important.  \n",
    "Malheureseument, la période d'extraction des tweets avec l'API ne s'étend qu'aux 7 derniers jours. C'est pourquoi nous utiliserons dans un second temps les tweets pour la construction de notre modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tweet):\n",
    "    data = {\n",
    "        'date': tweet['created_at'],\n",
    "        'text': tweet['text']\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION :** le code pour récupérer les tweets financiers des 7 derniers jours prend un dizaine de minutes à tourner. Si vous souhaitez vous épargner ce temps d'attente, vous pouvez directement passer à l'étape suivante en important la base de tweets non nettoyée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN=keys['database']['bearer_token_twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel à l'API\n",
    "endpoint = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "headers = {'authorization': f'Bearer {BEARER_TOKEN}'}\n",
    "params = {\n",
    "    'query': '(VIX OR S&P 500 OR CBOE OR investement OR stock market OR Federal Reserve Bank OR stock price OR inflation OR bonds ) (lang:en)', # mots en rapport avec la finance qui doivent être présent dans les tweets\n",
    "    'max_results': '100', #  on récupère 100 tweets à chaque appel à l'API. C'est le nombre maximal de tweets que l'on peut obtenir avec notre compte.\n",
    "    'tweet.fields': 'created_at,lang'\n",
    "}\n",
    "\n",
    "\n",
    "dtformat = '%Y-%m-%dT%H:%M:%S.000Z'  # le format de la date recquis par l'API Twitter\n",
    "\n",
    "def time_travel(now, mins):\n",
    "    '''\n",
    "    Permet de modifier la date en voyageant de 'mins' minutes dans le passé \n",
    "\n",
    "    'now : str \n",
    "        date actuelle\n",
    "    \n",
    "    'mins : int\n",
    "        minutes à soustraire\n",
    "    '''\n",
    "\n",
    "    now = datetime.strptime(now, dtformat)\n",
    "    back_in_time = now - timedelta(minutes=mins)\n",
    "    return back_in_time.strftime(dtformat)\n",
    "    \n",
    "now = datetime.now() # date actuel\n",
    "last_week = now - timedelta(days=7)  # date de fin\n",
    "now = now.strftime(dtformat)  # convertit la date au format de l'API\n",
    "\n",
    "\n",
    "now=time_travel(now,60)\n",
    "df_tweet = pd.DataFrame()  \n",
    "\n",
    "time.sleep(20) # on attend 20s pour déclencher l'appel à l'API\n",
    "\n",
    "\n",
    "while True:\n",
    "    if datetime.strptime(now, dtformat) < last_week:\n",
    "        # si on atteint les 7 jours, on sort de la boucle\n",
    "        break\n",
    "    pre60 = time_travel(now, 30)  \n",
    "    \n",
    "    params['start_time'] = pre60\n",
    "    params['end_time'] = now\n",
    "    response = requests.get(endpoint,\n",
    "                            params=params,\n",
    "                            headers=headers)  # envoie de la requête\n",
    "    now = pre60  # on voyage 60 min avant pour récupérer un maximum de tweet sur chaque heure\n",
    "\n",
    "    # on ajoute les tweets à notre df\n",
    "    for tweet in response.json()['data']:\n",
    "        row = get_data(tweet)  \n",
    "        df_tweet = df_tweet.append(row, ignore_index=True)\n",
    "\n",
    "df_tweet.to_csv(\"Data/tweet_not_cleaned.csv\",index=False) # enregistrement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing des Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des tweets pas nettoyés :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=pd.read_csv(\"Data/tweet_not_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on supprime tous les pseudos qui comportent le mot vix car cela ne correspond pas aux tweet sur la finance\n",
    "for index,row in df_tweet.iterrows():\n",
    "    tweet=row[\"text\"]\n",
    "    real_pseudo=[]\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        if '@' in word:\n",
    "            real_pseudo.append(word)\n",
    "\n",
    "    vix_in_pseudo=[\"vix\" in pseudo.lower() for pseudo in real_pseudo]\n",
    "\n",
    "    if True in vix_in_pseudo:\n",
    "        df_tweet.drop(index, inplace=True)\n",
    "\n",
    "df_tweet.reset_index(inplace=True,drop=True)\n",
    "\n",
    "## on modifie la date \n",
    "df_tweet[\"date\"]=df_tweet[\"date\"].apply(lambda x : x[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction qui nettoie les tweets\n",
    "\n",
    "def transform_tweet(texte):\n",
    "    texte = texte.lower() # mettre les mots en minuscule\n",
    "    #retirer les liens\n",
    "    for item in re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # enlever le retour à la ligne\n",
    "    texte = texte.replace(\"\\n\", \" \").replace(\"\\r\", \"\") \n",
    "    # supprimer \",\", \"!\", \"?\", \"%\", \"(\",\")\",\"/\",'\"', \"$\",\"£\", \"_\", \"-\", \"+\", \"*\", \"µ\", \":\",\"&,\"§\" \n",
    "    texte = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"\\$\\£\\+\\*\\µ,\\:\\&\\§]\", \" \", texte) \n",
    "    # retirer les hashtags #\n",
    "    for item in re.compile(\"([#]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les apostrophes '\n",
    "    for item in re.compile(\"([\\’])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    for item in re.compile(\"([\\'])\").findall(texte):\n",
    "        texte=texte.replace(item, \" \")\n",
    "    # retirer les points de suspension\n",
    "    for item in re.compile(\"([.]{1,5})\").findall(texte):\n",
    "        texte=texte.replace(item, \"\") \n",
    "    # retirer les personnes tagées\n",
    "    for item in re.compile(\"([@]\\w+)\").findall(texte):\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retirer les adresses mail\n",
    "    for item in re.findall('\\S+@\\S+', texte) :\n",
    "        texte=texte.replace(item, \"\")\n",
    "    # retire les mots contenant des chiffres\n",
    "    texte = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", texte)\n",
    "    # retirer les emojis\n",
    "    for item in demoji.findall(texte):\n",
    "        texte=texte.replace(item,\"\")\n",
    "    return texte\n",
    "\n",
    "df_tweet[\"tweet_clean\"]=df_tweet[\"text\"].apply(lambda x : transform_tweet(x))\n",
    "\n",
    "## on retire les stopswords\n",
    "\n",
    "df_tweet[\"tweet_clean\"] = df_tweet[\"tweet_clean\"].apply(retrait_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on \"stem\" les tweets\n",
    "df_tweet[\"tweet_stemmed\"]=df_tweet[\"tweet_clean\"].apply(lambda x : ' '.join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons dès à présent finit le nettoyage des tweets. Nous les enregistrons dans le fichier .csv pour pouvoir les utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.to_csv(\"Data/tweets.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "809c05bf197248844069ca010303a407c57ade7aa07b41750c0b87641ddc144b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
